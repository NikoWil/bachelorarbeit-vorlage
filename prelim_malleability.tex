\subsection{Malleability}
\label{prelim: malleability}
In this work, we follow the classification of \cite{feitelson1997job} regarding parallel jobs. A job is \textbf{rigid} if it has a fixed number of required PEs which is hard-coded in the application. This number stays the same between runs. We call a job \textbf{moldable} if the number of PEs is variable and can be set at application start but remains fixed within any one run. An \textbf{evolving} job is one where the required number of PEs changes during execution and where these changes are initiated by the user. If the number of PEs changes during execution with the changes initiated externally, we call a job \textbf{malleable}.
Malleability can be defined more generally as the ability to deal with changing resources, not only PEs \cite{sonmez2007scheduling}. In practise, we see that most jobs running on supercomputers follow the moldable model \cite{cirne2001model}. The moldable model is also supported by programming environments such as MPI \cite{hungershofer2004combined}. \\
Systems that utilize malleable jobs have been shown to be highly efficient \cite{feitelson1997job}.
They achieve this in multiple ways.
First, they allow for efficient scheduling, as the scheduler can reevaluate and change previously made decisions \cite{sonmez2007scheduling}. This allows to resolve the conflict in scheduling between throughput and response latency, where low latencies come with the need to keep spare resources on hand instead of fully utilizing them \cite{feitelson1997job}, \cite{hungershofer2004combined}.
Second, they allow applications to utilize additional resources as they become available, leading to improved performance \cite{hungershofer2004combined}. Lastly, \cite{buisson2005framework} make the case that malleable applications are more fault-tolerant which is of increasing importance as applications become more parallel. \\
While malleable jobs are desirable from a scheduling and administration perspective, they are not popular with the user side, as they impose additional complications \cite{feitelson1997job}. The effort required to make any one application malleable varies depending on the problem. In case the problem at hand is easily split into independent small subtasks, we can use a central work queue from which other PEs can receive new tasks as needed \cite{feitelson1997job}, \cite{tucker1989process}. This approach allows us to redistribute PEs to other jobs in between tasks. It is however limited by the central work queue which tends to be a bottleneck and makes strong assumptions about the structure of our problem. Alternatively, in data driven applications, we may have distributed data structures that are redistributed as the number of available PEs changes \cite{feitelson1997job}. This is more complicated, though, and is an expensive operation which should not be performed too often. Lastly, \cite{schreiber2021scalable} showed for SAT that a portfolio approach is easy to adapt to a malleable environment. Loosing single workers may slow down progress but completeness is preserved. Additionally, a periodic exchange of knowledge can benefit the remaining workers even as some solvers terminate. To sum it up, making an application malleable is highly dependent on the specific problem and only easy in cases that are trivial to parallelize. \\
Due to the inherent complexities, in practice there are only few malleable applications. This may change with the introduction of malleable SAT solvers such as Mallob \cite{schreiber2021scalable} and Paracooba \cite{heisinger2020distributed}. SAT forms an important building block in many applications. By presenting an easy to use interface while using malleability internally, a SAT solver may unlock the benefits of malleability for at least part of an application's work.

\begin{comment}
\cite{feitelson1997job} gives us a classification of parallel jobs
- rigid: jobs which have a fixed number of required processors, this number stays fixed across iterations
- moldable: jobs which require the number of processors to be fixed during execution, but may run on any number
- evolving: a job whose needs change over time, it requests a number of parallel units for each phase
- malleable: a job who can utilize a changing number of parallel units, the number can be changed externally and the job will adjust

In evolving jobs, the different number is set by the job, in malleable jobs the system sets that changing number.
- malleable systems have been shown to be highly efficient
- the model is not very popular with users
- malleability is less convenient for application writers, as it exposes them to the variation in available resources

- efficiency:
- if we are not malleable we face a choice of either leaving PEs unassigned (and underutilized!), reducing throughput
- or alternatively we may fully utilize all PEs, worsening latencies for new jobs

- they propose models to make malleability work on the application side: work pile
- multiple independent cores get their task from the pile of work, independently and in unknown order
- PEs may be reassigned to a different job in between tasks
- problems:
- the central work queue is a bottleneck
- what do we do if we want to reassign a PE in the middle of a task?

- alternatively we can redistribute our data structures upon changing number of PEs
- this is an expensive operation and should be done as little as possible!

\cite{tucker1989process}
- also propose a big queue of tasks from which single processes take some to work on

\cite{sonmez2007scheduling}
- malleability is the ability to deal with changing resource allocation during execution (not only processors)
- for the scheduler: change the decisions about resource allocation after they have been made
- also helps performance as applications may be able to gain resources during runtime
- malleability opens ways to extract more performance from clusters

\cite{hungershofer2004combined}
- in parallel systems, users want fast response times, administrators want high throughput and utilization
- these needs contradict each other, malleable jobs can help!
- programming models such as MPI support the moldable model
- show that malleable jobs can be scheduled more efficiently

\cite{cirne2001model}
- most jobs on supercomputers are moldable
- this means about 98\% of jobs run on supercomputers

\cite{buisson2005framework}
- note that using more resources in parallel decreases mean time between failures
- the ability to adapt to such failures makes programs more resilient
- require the program itself to make observations on the availability of resources
\end{comment}
