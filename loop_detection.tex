\subsection{Loop Detection}
\todo{Make sure to always talk about isomorphic task networks instead of equivalent? Quote \cite{holler2021loop}. Define isomorphism!}
\todo{quote about loop detection in graph search?}
\todo{quote about distributed loop detection in graph search?}
This section will discuss loop detection as it is used in (TO)HTN planning in general. It will start with an overview over loop detection in other HTN planners in section \ref{ld - history others}. This is followed by a discussion of the simplifying assumptions we can make for TOHTN planning (section \ref)
- distributed loop detection (communication and merge operations become important!)
- perfect loop detection
- probabilistic loop detection (approximate membership query)

- \cite{magnaguagno2020hypertension} domains can have introduce infinite loops, without some kind of loop detection we loose completeness (and performance)


\subsubsection{Loop Detection in Other HTN Planners}
\label{ld - history others}
Loop detection in HTN planning is a recent phenomenon and was introduced in 2020 by the HyperTensioN planner (\cite{magnaguagno2020hypertension}) with the so-called 'Dejavu' technique. Dejavu works by extending the planning problem, introducing primitive tasks and predicates that track and identify when a particular recursive compound task is being decomposed. These new primitive tasks are invisible to the user. Information about recursive tasks is stored externally to the search as to not loose it during backtracking. Dejavu comes with performance advantages and protects against infinite loops. However, as Dejavu only concerns itself with information about the task network but ignores the world state it may have false positives. This was also noted by \cite{holler2021loop} and means that HyperTensioN is not complete. \cite{holler2021loop} further nodes that the loop detection is limited in that it only finds loops in a single search path but cannot detect if multiple paths lead to equivalent states. \\
In response to \cite{magnaguagno2020hypertension}, loop detection was introduced to the PANDA planner in \cite{holler2021loop}. Similar to HyperTensioN, PANDA keeps it's loop detection information separate in a list of visited states, $\mathcal{V}$. Search nodes, identified by a tuple $(s, tn)$ of world state $s$ and task network $tn$, are only added to the fringe if they are not contained in $\mathcal{V}$. To speed up comparisons, $\mathcal{V}$ is separated into buckets according to a hash of $s$. To then identify whether $tn \in \mathcal{V}[s]$ multiple algorithms are proposed. In the sub-case of TOHTN planning, both an exact comparison of the sequence of open tasks as well as an order-independent hash of the open tasks, called \textit{taskhash} are used. Similar to HyperTensioN, using a hash to identify equal task networks can lead to false positives and an incomplete planner. Both hashing-based and direct comparison as used in PANDA have a performance cost linear in the size of $tn$. The loop detection in PANDA improves upon the one in HyperTensioN insofar as it is able to detect loops where equivalent states where reached independently.

\begin{comment}
- loop detection in HTN planning is relatively new, being introduced by \cite{magnaguagno2020hypertension} and \cite{holler2021loop}
- multiple other planners do perform loop detection
- HyperTensioN \cite{magnaguagno2020hypertension}
	- transform the domain, adding primitive tasks that mark when a particular, recursive, task is being decomposed
	- use an external cache to store this information (else it would be lost in backtracking)
	- can also fall back on comparing the full search state (needed, if no parameters are present in the recursive task, to identify specific instances?)
	- limited, as it does not consider the world state, only the decompositions!, planner no longer complete \cite{holler2021loop}
- PandaGBFS \cite{holler2021loop}
	- search nodes are stored in a fringe
	- separate visited list $\mathcal{V}$
	- only add nodes to the fringe if they are not in $\mathcal{V}$
	- a node is identified by a pair $(s, tn)$, $s$ state, $tn$ task network (as in crowd!)
	- $\mathcal{V}$ is separated in buckets identified by states, access bucket based on 64 bit integer hashing (of $s$?)
	- problems with task network isomorphism, as Panda deals with HTN instead of only TOHTN, ordering relations matter!
	- simplifying isomorphism -> accept false positives (but not false negatives!), may cost completeness
	- exact isomorphism for TOHTN (TN -> sequence of tasks, simple)
	- hash (for TOHTN) that discards ordering information (nicer for HTN), only tracks multiplicity of all open tasks
	- hash can be computed in linear time (as no ordering)
	- hash is used both for making search more efficient (less task network comparisons!) and as an overapproximation
\end{comment}

\subsubsection{Assumptions in Loop Detection for CrowdHTN}
\label{ld - tohtn simplifications}
To design the loop detection in CrowdHTN, we have both simplifying and complicating assumptions that we will discuss here. \\
While both PANDA and HyperTensioN are HTN planners, CrowdHTN concerns itself only with TOHTN planning. As a result, the remaining task network can be represented as a sequence of open tasks with the ordering constraints implicit in how the sequence is stored.
\todo{refer to the implementation part where the open tasks are represented as a sequence} \\
Crowd identifies search states as a tuple of $(s, tn)$ of world state $s$ and task network $s$, similar to PANDA and uses hashing to efficiently identify duplicate search states. As tasks of equivalent task networks are always in the same order, we can incorporate that order into our hash of $tn$ to reduce the number of collisions compared to PANDA's \textit{taskhash}. This will increase performance where we fall back to comparisons in case of collisions and reduce our false-positive rate in case we use probabilistic loop detection. 
\todo{Refer to the implementation and how we reduce hashing to O(1) time} \\
Both PANDA and HyperTensioN are sequential planners whereas CrowdHTN is highly parallel. This adds an additional design constraint to our loop detection. If we want to efficiently share information about visited states using the full state information becomes infeasible as full states would have to be communicated. If we perform loop detection only locally, we predict to suffer from decreased performance the higher the degree of parallelism. I.e., if two branches of our search tree contain the same search node, the chance that those two branches are encountered on different processors is higher the more processors we have.

\begin{comment}
	- keep the part where we store loop detection information externally
	- be sure we can always assume total order (and keep the ordering information to reduce false-positive rate!)
	- keep completeness of the planner
	- improve on the performance for hashing!
	- 
\end{comment}


\subsubsection{Perfect Loop Detection}
One simple way to perform loop detection which is similarly used in PANDA (\cite{holler2021loop}) is to use a hashset of visited states. The implementation in Crowd is slightly different from PANDA in that we use one combined hash for both world state and open tasks. Other than PANDA, CrowdHTN does incorporate the order of tasks into the hash, which should reduce collisions and makes the two levels of hashing less needed. \\
Using hashes combined with a full comparison provides us a perfect loop detection, i.e., neither false positives nor false negatives exist. This makes it a useful technique to benchmark other loop detection methods. However, both in the sequential and in the distributed case this technique suffers from performance problems. \\
In case of hash collisions, we have to fall back to a full comparison of world state $s$ and open tasks $tn$. While $s$ is bound in size by the total number of predicates, the size of $tn$ is effectively unbound \todo{quote about max. exponential depth!}. Additionally, we have to keep both $s$ and $tn$ around for all nodes ever encountered, increasing the memory footprint of our program.
In case of distributed loop detection, communicating full states would lead to a large communication overhead. Especially, we can expect each node to be larger than those sent as work packages, as those are optimized to have a small $tn$ which would not hold for nodes encountered in our loop detection.

\begin{comment}
	- Additionally, there is the full comparison for the world state
	- in practise this can be a problem, at the same time asymptotically it should not matter as it will be dwarfed by the 
	\todo{get some data on how many nodes share a world state on average as well as sizes of world states -> number of hash operations per world state!}
	- and the full comparison of open tasks
	- we cannot just free the open tasks and world state that are no longer needed! Both time and memory footprint are worse
	- worse memory and time complexity
	- however, it is a useful benchmark as to how many loops we *should* expect
\end{comment}

\subsubsection{Bloom Filters}
\todo{Write about why a quotient filter is not the solution! (or at least why it was not chosen)}
Quotient filter:
- do we need the original elements to re-insert them?
- do we need to communicate whole hashes to combine filters? (worse communication size!)


Advantages:
- communication takes less effort (a lot lower size of data!)
- hashing in O(1)? Or at least in a lot easier
- we can pre-hash the open tasks (can be of exponential depth (requires a good argument, as the path down the task network could be of width 1?) \todo{do the maths}), thus turning hashing into O(1)
- in case of hash collisions we need to walk the whole sequence of open tasks
- in practice this is even worse, as our open tasks are saved in a tree-like manner to allow sharing of the tasks between search nodes
- this means we wildly jump through memory for hashing, adding another constant factor
- we could save on this constant factor by duplicating the open tasks for each node and saving them sequentially, but leading to a lot higher memory footprint (might be quadratic compared to what it is already (if any fixed fraction of nodes have at least 2 children, maybe? \todo{some math}))
- compromise between performance and false-positive rate (better for correctness than just comparing a single hash)

\subsubsection{Distributed Loop Detection}
- two new considerations
	- memory footprint becomes more important for communication (effectively an all-to-all operation -> quote Sanders' book!)
	- efficient merge of loop detection data is needed
- communicating everything might still be inefficient for bloom filters ()