\section{Improvements to CrowdHTN}

\subsection{Reducing Memory Consumption}

\subsubsection{Efficiently Storing the Preceding Plan}
- no longer use a vector of USignature
- instead use an ImmutableStack<USignature>
- this way we can go from linear overhead to constant memory per search node!
- actually, with A star or loop detection via hashset it gets like, really complicated
- argh
- still, lots of sharing!
- lets protocol the thing (memory logging overall)

\subsubsection{Reducing Copies of the World State}
- if the next open task is an action, there is only a single possible way to resolve it
- in other words, a search node whose next open task is an action always has zero or one children
- this is not a very interesting node, either the preconditions hold or they do not
- such nodes are no longer represented explicitly
- instead, apply all actions from the sequence of open task up until the first abstract task is encountered
- then also resolve that abstract task
\todo{Add graphic showing how many fewer nodes are represented!}

\subsubsection{Only Saving 'Potentially Interesting' Nodes}


\subsection{Efficiently Hashing Nodes of the Search Graph}
\todo{Time is not actually constant for world state -> what is the expected time?}
Overview:
\begin{itemize}
	\item the hash of a node consists of two parts
	\item 1: the hash of the open tasks
	\item 2: the hash of the world state
	\item We do not care about the hash of the preceding plan. Nodes with open tasks and world state equivalent have equivalent plans leading to a goal (somewhat similar to the Nerode relation) and we do not care about optimality. How he reach this point with equivalent remaining options is thus not of interest to us
	\item The hash of the world state can be large, but the number of elements is ultimately bound for any particular instance
	\item The length of the preceding plan is unbounded
\end{itemize}
Open tasks:
\begin{itemize}
	\item Care needs to be taken to use an order independent hash function for the world state, as the underlying representation as a set does not guarantee us any particular iteration order, especially between nodes (might depend on the order of things inserted into the set!)
	\item Alternatively we could have chosen some other ordered representation, e.g. maintain the world state as a sorted list of predicates with a defined order. This would imply extra work we are unwilling to do.
	\item For open tasks, we save not only the task itself but additionally the hash of all open tasks from first to current one
	\item The order in which we hash open tasks is from oldest to newest one
	\item On applying a Reduction, we push the new open tasks onto the open task stack and compute each of their hashes combined with their predecessors. Each of these hashing operations is completed in O(1)
	\item Effectively, this means that each task is hashed exactly once
	\item As we already have to push each task onto the open tasks, inserting an additional O(1) operation on pushing does not change the asymptotic runtime
\end{itemize}
World state:
\todo{this is not implemented yet, take care to actually bring it into Crowd}
\begin{itemize}
	\item In section XXX \todo{add section, refer to it here} we discussed how each copy of the world state can be shared by many search nodes to reduce the memory footprint
	\item Instead of hashing the world state each time on demand, we can store a shared tuple of world state and it's hash
	\item This way, we only hash the world state once, reusing the computation
	\item Is this actually a time saving?
	\todo{Only put node into loop detection as it is explored, so we can always keep exploring the local node (the node we are colliding with may be arbitrarily far back in our fringe)}
	\todo{Evaluate how many world state hashes we actually compute in both cases. Hashing a world state of a node which is ultimately not explored (and which is otherwise not needed) is wasted.
	Number of computations on lazy hashing: number of nodes created - number of nodes remaining in fringe on plan found}
	\item Future work: the hash of the world state is order independent (sum of squares of individual hashes), to not worry about forcing any fixed iteration order. Utilize this and wrapping maths to hash only the differential of 
\end{itemize}

\subsection{Preceding Plan}
\todo{Section no longer valid for current crowd}
In the initial implementation each node stored the full preceding plan as a sequence of all reductions that were applied so far. This leads to roughly quadratic overhead (sum over 1..n, only roughly as not each step increases the length. Wait, is it roughly, then? Probably, as the fraction of steps that increment the preceding plan should be kinda constant)
This duplication was not needed. The newer implementation instead stores an optional<reduction> in each node. I.e., the preceding reduction is stored if one exists, nothing if there isn't one. When the preceding plan is needed, either for communication or to extract a plan, the current search path is iterated and all reductions are accumulated.

\subsection{Lazy Instantiation of Child Nodes}
lazy instantiation works on the basis of finding all free variables of a method and creating Reductions based on all possible combinations

Initial implementation: instantiate all possible reductions, filter out any with not fulfilled preconditions,then shuffle them

Problems: this spends both time and memory instantiating reductions that might never be needed for the rest of the search
We effectively save not only the current path, but also follow all possible branches to a distance of 1

Solution: lazily create reductions as needed
How to do this (first way):
adapt the argiterator from Lilotane into the CrowdHTN code base. Adapt it to only substitute the arguments that are not already determined by the corresponding task (arguments)

To achieve randomization:
each domain is iterated to create the substitutions
Each time we build such an iterator, we randomize the order within the domains for this specific iterator
This will lead to different orders

Further ideas:
each time domains 1..k have been fully iterated, increment k+1 by 1, then shuffle the order of domains 1..k

For n total domains, each time domains 1..n-1 have been iterated, remove the current value from domain n, then shuffle all domain orders

Compare the runtimes of eager and lazy instantiation, check at which point it is worth it to incur the (potential) additional overhead of lazy instantiation
Compare on multiple domains?
Check different metrics for comparison (size of domains, number of parameters, number of potential children (product of sizes))

Potential problems:
We need quite a bit of state (domains, current index into each domain) to perform lazy instantiation
The order is not truly random. We iterate some domains faster/ more often than others. What if the important change is in a domain which is iterated slowly? More random order makes this easier

A potential solution: space filling curves
Advantages: little state (can just be incremented), iterates all dimensions equally
Disadvantage: fixed order. With shuffling within each domain might be random again

Space filling curves come with the restriction of being strictly continuous
We do not need this property. All we are interested in is an easy to compute fixed order in which the whole space is iterated where each permutation is hit exactly once
We want only self-avoiding curves, to not hit any instantiation twice (could loop detection just fix that? But it would be a bad fix)

\subsection{Using Domain Meta-Information}
inspiration taken from HyperTensioN

taking preconditions of tasks and lifting them up into methods
Only do this if the precondition is guaranteed to also apply to the method (cannot be achieved before the respective task?)
This allows us to stop exploring paths earlier

\subsection{Global and Memory Efficient Loop Detection}
So far: each worker has a hash set of each node ever encountered (note: we define node equality through the sequence of open tasks and the set of predicates that is the world state. Depth in the graph and preceding plan are ignored)
Advantages: allows for perfect local loop detection, as we can always fall back to equality checks in case of hash collisions.
Problems: This approach leads to a massive memory overhead, as the world state is duplicated countless times. This also leads to increased run times just to manage the growing hash set.
This is also not suited for global loop detection. Global loop detection would need some mechanism to communicate seen nodes. Nodes are too large to communicate all of them, though. If we only communicate some nodes instead of all we run the risk of a node "going past" that barrier of known nodes and still exploring the swathes of known nodes beyond the "barrier"
TODO: what about open tasks (memory consumption)? Or was that an ImmutableStack with little overhead?

Idea: drop the precondition of perfect loop detection with no false positives.
For loop detection use a bloom filter with a set of hash functions for search nodes.
This comes with a fixed, configurable memory overhead per worker. As a result, overall memory consumption should
- drop
- be more predictable
To communicate nodes we can just communicate the bit array that makes up our filter and combine them via bitwise OR. This ensures that communication volume is also fixed. It is independent of both size of nodes (i.e. length of open tasks sequence) and number of explored nodes (since last round of communication).
In addition to communicating the bloom filter we communicate the number of newly added nodes. Then each worker knows an upper bound for the total number of nodes that are part of it's bloom filter (might be an overestimation if two workers insert identical nodes as the node would be counted twice). As bloom filter performance/ false positives degrade with the number of elements inside it, we can restart our work depending on the number of elements in the bloom filter.

- loop detection information is shared at most every second (or however long it takes for the communication to complete)
- as a result, when the root node increases it's version, a broadcast is started that communicates the new version to everyone else
- the version in normal messages is still kept. In case a message arrives before the version broadcast, the version is increased even earlier. New nodes likely get their version with the first work package.

\subsubsection{Restarts Under Loop Detection}
- bloom filters are of fixed size. If we propose a maximum false-positive rate, they will fill up at some point
- this necessitates the need for restarts
- bloom filter architecture: for local loop detection, use an expanding bloom filter (citation!)
- this allows us to not incur any restarts due to local state
- for global loop detection, we use a fixed-size bloom filter
- if a node is encountered twice for local loop detection, add it to the global loop detector, to communicate

- we know that in mallob, the tree of workers is always a 'full' tree, except for the lowest level
- i.e. we may be missing some nodes to the right end of the lowest level
- especially, the root node always survives
- as a result, the root node always receives all the messages about shared loop detection data
- i.e., if the global loop detection bloom filter on any node is full, it follows that the global loop detection bloom filter on the root node is also full
- the opposite does not hold (i.e. a new node enters just as the root node bloom filter fills up)
- as a result, we can simply delegate the question of whether to restart to the root node and have it done in a centralized way, simplifying our communication patterns

\subsubsection{Reaching Consensus on Search Version}
- each search version corresponds to a restart with subsequent doubling in size of the global loop detector
- keeping versions in sync is of importance
- we do not want to loose any work
- we do not want to insert search nodes from a wrong version into our bloom filter (especially old node into new filter, other way around is discarded either way)
- each message between workers is extended by their internal version
- if the version of a received message is higher than the internal version, the internal version is increased and then acted accordingly
- new workers get updated to the current version the first time they ask for a work package
- work requests and responses do not suffice as a version updating mechanism, as work packages may be arbitrarily large and messages arbitrarily rare
- 

\subsubsection{Completeness Under Loop Detection}
- any single iteration may be incomplete, e.g. if initial node and goal node hash to the same values
- with restarts, we have uniform hashes, compute the chance of colliding each time
- this necessitates changing seeds with each restart
- soon, we will not collide
- yay, plan can be found!

- we do loose termination
- so far, termination could be known if the search tree gets too deep (cite limits known from other TOHTN paper)
- however, this is infeasible, as RAM is actually finite (refer to known branching factors!)
- i.e., we do not know the hash of a goal node, as a result we cannot exclude the possibility of it simply being cut off each time so far
- we also do not know the hash of all nodes just before a goal node, etc (the path could always only be a single node wide)
- as a result, we cannot terminate at any point, as we cannot exclude the plan hiding in some always-cut-off part of the search space
- loosing termination is not a big deal anyways, I guess?

