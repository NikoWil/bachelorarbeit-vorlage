\begin{comment}
\subsection{A Common (TO)HTN Interface}
- be able to mix grounders, pruners, planners - maybe even loop detection!
- be able to combine different pruners - use a lifted pruner early and then plug in a grounder and grounded pruner afterwards
- PANDA seems to try to provide this
- quite successful regarding the input format for HTN planning which finally makes planners easily comparable, advancing the state of the art
- sadly not as successful for the rest of planning
- we believe their to be great potential in 
\end{comment}

\begin{comment}
\subsection{Combine Pruning Approaches}
- HyperTensioN and Panda base their pruning on different paradigms
- HyperTensioN's pruning could be extended to work on grounded instances where it would have even more information available
- both these approaches are structurally different and could thus be used to compliment each other
- whether the better pruning would offset the increased runtime cost remains to be seen
\end{comment}

\subsection{Advanced Dynamic Pruning}
In section \ref{improv: completeness} we show that the current state of the art in loop detection can detect some but not all cases of recursion as it is present in hierarchical planning problems. We further show that this not only negatively affects the performance of our planners but may make planners based on heuristic DFS incomplete. \\
To deal with such cases, we could take the idea of detecting duplicate search nodes and generalize it into a notion of dynamic pruning of uninteresting nodes. Uninteresting here means that exploring this node will give us no new information about our planning problem. A duplicate search node is uninteresting as we have explored it before, obtaining all information that is available. However, a search node may also be uninteresting if it's open tasks is built such that we are guaranteed to perform unnecessary work when exploring it. Such a set of open tasks may contain a sequence of $k$ times task $t$, where the resolution of $t$ may only create actions and more instances of $t$ and affect at most $l$ predicates. If $k > 2^l$, then extending this sequence by more instances of $t$ is of no benefit. We could only create more instances of $t$ and resolving all open $t$ via actions would lead us through duplicate world states, performing unnecessary work. \\
More research would need to be put into detecting such cases to perform more intelligent pruning of search nodes. If done successfully, heuristic DFS may regain completeness if combined with this new scheme.
\begin{comment}
- some planners are known to not be complete
- some planners can easily be shown to be complete
- we make a case in section \todo{Ref where we destroy our own heuristic and PANDA} that heuristic planners may not be complete
- to catch these cases planners may have to get more intelligent about which parts of the search space to cut off
- else we may have to adapt our search algorithms to enforce completeness at another level
- similarly we may achieve completeness by simply cutting off the search once we get too deep \todo{cite exponential max depth}
- however, this may be theoretically complete but also hits us with the full expense of an EXPSPACE-hard problem, i.e. we may never realistically reach this condition (except on very small problems)
- similarly it is not clear whether an algorithm like heuristic + plan length search gives us completeness in feasible time or more theoretically \todo{Compare with results once they arrive}
- HyperTensioN, a decidedly non-complete planner is also the best-rated planner in the IPC 2020
- maybe we need to have a full conversation on how much completeness we actually want/ need (or two separate categories of planners between 'HTN as a way to provide advice into planning domains \todo{quote Erol, I think?}' and 'HTN for the full power it provides')

- new loop detection techniques leading to planner completeness
\end{comment}

\begin{comment}
\subsection{Lifted Parallel Search}
- the Crowd way of performing relatively simple search does not work out in the end
- we can still see some scaling for the parallel case
- lifted planning could massively shrink our search space (by an exponential factor in the number of nodes!)
- lilotane provides much intelligence on how to perform lifted search
- same as HyperTensioN
- a lilotane-like behavior may be the best from a practical perspective, as it is more consistent
- a search-based formulation may be easier to both parallelize and adapt to a malleable context - any search may be plugged in where CrowdHTN currently resides
\end{comment}

\subsection{Improvements Unlocked by a Shared Memory Implementation}
- we have decided to stay with CrowdHTN's way of performing grounding on-demand and just in time, as it lends itself particularly well to a malleable environment
- specifically, having to perform a big chunk of work that is not yet possible to lead to a plan would impose a problem on the efficiency of new and short-lived workers
- in a shared-memory environment, this could be done once by the root and then re-used by other workers, making better pruning and heuristics such as in PANDA available

\subsection{Global Loop Detection}
- we show how a global loop detection mechanism in (TO)HTN planning could work
- so far, we use a very simple heuristic (encounter a node twice) for which nodes to share
- more intelligent heuristics are probably helpful
- we are interested in search nodes often reached, less in recursive tasks (can be resolved locally)

- how about we insert nodes once we backtrack past it
	- this information may even be kept through restarts
	- would necessitate us to keep old search nodes around and re-engineer our planner

\subsection{Intelligent Restarts}
In both \ref{ld - completeness} and \ref{improv: completeness} we argue for the use of probabilistic restarts to guarantee the completeness of CrowdHTN. We perform restarts with probability $\frac{1}{t}$ at second $t$. This is only one of the possible ways to use restarts. \\
At the same time, restarts and restarting strategies to increase solver performance have played a role in SAT solving since the 1990s.
\cite{langley1992systematic} suggested the use of iterative sampling in AI planning systems, repeatedly exploring random paths up to a depth limit and restarting if no solution was found. 
Their work was adapted by \cite{crawford1994experimental} who employed the iterative sampling strategy for SAT solving. As a general restart strategy for increased performance, the Luby sequence was developed \cite{luby1993optimal} and has by now been adapted by SAT solvers \cite{huang2007effect}. By now a wide array of different restart strategies have been tried in SAT solvers. There are both static restart strategies, such as uniform intervals and based Luby schemes and dynamic strategies which incorporate run time information into their decision \cite{biere2015evaluating}. \\
We are eager to see how incorporating such restart schemes may affect the performance of hierarchical planners.

\begin{comment}
In SAT solving, restarts have long been an important part of solvers to increase their performance.

- we perform restarts according to the harmonic series
- SAT solving already knows restarts, explore inner-outer and luby sequence for potentially better performance

- \cite{langley1992systematic} suggests iterative sampling as an extreme form of restarts
- \cite{crawford1994experimental} uses restarts via iterative sampling for SAT solving
- \cite{gomes1998boosting} use restarts in planning, argue their use in combinatorial search
- \cite{biere2015evaluating}
- optimal restart points may be unknown
- frequent restarts may help in learning
- long times may be needed to find solutions
- show that luby is quite good
- different restart policies:
- static: uniform, geometric, luby
- dynamic: agility - prohibit restart depending on status, 
- \cite{luby1993optimal} propose the luby sequence as a strategy
- \cite{huang2007effect} use luby sequence in SAT
\end{comment}