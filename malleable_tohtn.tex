The goal of this section is to describe how we adapt it to be a malleable TOHTN planner by integrating it with Mallob, preserving both the completeness and scalability of CrowdHTN in the process. Before we get into the details, let's recall that CrowdHTN is already a moldable program according to the definition introduced in section \ref{prelim: malleability}, i.e., it may utilize any number of PEs as long as that number stays fixed during the run. We will now introduce a design that extends the parallel capabilities to achieve malleability. For this we need to address three main concerns.
\begin{itemize}
	\item Distributing the job information
	\item Integrating new PEs into a running job
	\item Dealing with PEs leaving the job while it runs
\end{itemize}
In the following sections we will address these problems in this order. Both distributing the job information and integrating new workers do not pose significant problems. Most time will be spent on the handling of disappearing PEs.
Due to the fact that we specifically integrate CrowdHTN with Mallob, in some parts we will have to refer to implementation details regarding how Mallob organizes the PEs assigned to a job as well as general message delivery.

\subsection{Distributing Jobs}
When a PE is assigned to a job, it needs to obtain a description of this job. In case of TOHTN planning, the choice is mostly between a lifted or ground TOHTN instance. Depending on pruning, a ground instance may be up to exponential in size \cite{behnke2020succinct}. Encoding and communicating such a ground instance would take up much time, which is why we decided to communicate our problem as a lifted instance. \\
With the lifted instance, we choose to simply take the textual hddl input (\cite{holler2020hddl}) and send it as-is. While this does occur the overhead of locally parsing the instance on each PE, communicating the parsed instance would involve re-encoding and effectively re-parsing it locally, too. \\
In malleable (TO)HTN planning there is a more general trade-off involved when it comes to precomputation. While parsing the instance is unavoidable, we can choose whether we want to spend time grounding and pruning our instance. It has been shown that grounding and pruning improve the planning performance (\cite{behnke2020succinct}) and allow for the computation of complex and good heuristics (\cite{holler2020htn}), but grounding, pruning and other precomputations are expensive operations themselves. As a result, a PE which is only assigned to our job for a short time may never perform any actual planning work before it is reassigned to the next job. For this reason, CrowdHTN takes an alternative path. The TOHTN instance is kept in lifted form. Instantiation is only performed as needed to explore the current search node. This allows CrowdHTN to start working immediately to utilize even short-lived PEs.
\begin{comment}
- grounded instance may be exponential in size
- exceedingly high communication cost
- lifted instance it is
- sending a parsed instance - we still need to encode and decode it for sending
- we would effectively have to build a parser for this
- we already do have a parser
- we simply encode the string and parse locally
- not a core issue to us, so we go for ease of implementation
\end{comment}

\subsection{Integrating New PEs Into Malleable CrowdHTN}
To integrate a new PE into a running TOHTN job, it needs both the general job description and part of the actual work to handle. In the previous section we explained how the job description is obtained, now we will focus on the work itself. \\
The efficient integration of new PEs into a running job is where work stealing shows it's strength. For work stealing, there is no functional difference between a PE which has locally run out of work and a new PE which has the job description but no work yet. Both will message other PEs at random to receive a new work package with no special handling required. As a result, a new PE can perform at full efficiency almost immediately, allowing our job to utilize resources as they become available.
\begin{comment}
- work stealing makes this easy
- there is functionally no difference between a new PE and a PE which has locally run out of work
- no special handling required at all

- only setup work: parsing, initiating heuristic
- CrowdHTN may be less efficient, but can work immediately, make use of very short-lived PEs
\end{comment}

\subsection{Handling PEs Leaving at Run Time}
The last challenge in designing a malleable CrowdHTN is the fact that PEs may disappear at any time. This represents a potential loss of information. The information loss presents itself in two ways. First, the loss of the local search fringe, if we do not communicate it to another PE and second, messages which may be lost in transit as their receiver no longer belongs to the same job. To deal with this, Mallob does allow us to detect locally when a PE is taken away from a job and additionally provides a message return mechanism. We will present our solutions to both cases with a focus on preserving the completeness property of CrowdHTN.

\subsubsection{Handling the Local Fringe}
When a local PE is unassigned from a job, we will loose the local search fringe. As Mallob signals a PE when it is unassigned from a job, we are however free to encode parts or all of the fringe and communicate them to another PE. This leaves us with a number of choices where we may trade-off data loss versus efficiency and communication. On this axis we discuss three choices
\begin{itemize}
	\item Encode and redistribute the whole local fringe
	\item Communicate the root of the local search space
	\item Communicate nothing, loose the local fringe
\end{itemize}

\paragraph{Encoding and redistributing the whole fringe}
Encoding and sending off the local fringe to another PE is, in a way, the easiest operation. No information is lost, preserving completeness in our planner. It does, however, come with a number of disadvantages. First, the local fringe may be arbitrarily large, especially considering that TOHTN planning is EXPSPACE-complete as seen in section \ref{prelim: tohtn complexity}. Encoding and communicating a large fringe is a very expensive operation which would increase the time from Mallob telling a PE to suspend itself until the PE actually is free for the next job. Second, receiving a large fringe would strain the memory of the receiving PE which may lead to deleting parts of it anyways to avoid crashes. Third, to avoid duplication of work as Mallob may reassign the PE to the old job, the local fringe would have to be cleared out. Doing so would weaken the effect of Mallob reassigning previously used PEs to the same job.
\begin{comment}
- the most complete operation
- nothing is lost
- nodes higher up in the tree of PEs may be more strained now (depending on the communication pattern)
- a very expensive operation
- take care to delete the local fringe to avoid duplication!
\end{comment}

\paragraph{Communicating the root of the local search space}
Instead of communicating the whole local fringe, we can simply encode the root node the local fringe emerged from. In a way, this search node represents a very efficient encoding of the local search space. As we would only communicate a single search node, this would be more efficient and could reuse the facilities we already have in place for work stealing. Similar to encoding the whole fringe, communicating only the root search node would lead to no information loss, preserving completeness. \\
While this approach is very efficient and avoids loss of information, it does suffer from duplicate work. As we loose the local fringe, we will have to re-explore it again. Additionally, other nodes may have received parts of the local search space via work stealing. These nodes will be re-encountered leading to further duplication. In this way, we would trade-off local performance for encoding and communication against global performance through duplicating parts of our search. Similar to the first case, we would either have to clear out the local fringe upon suspension, decreasing the gains of PE reuse, or accept potential further duplication of work.\\
Implementing global loop detection as we propose in section \ref{improv: loop detection} would further complicate matters. Upon suspension of a PE, we could leave the global loop detection unaffected. This might lead to some losses in our search space as nodes will not be reexplored but may also help the search on our other workers which can still profit from being aware of common loops and prominent search nodes. Alternatively, we could remove the global loop detection data of our suspended PE from all other PEs. As more than one PE may have committed the same search node to global loop detection and due to the way bloom filters work, this brings its own problems. Namely, it would degrade global loop detection performance as we may delete more search nodes from our filter than strictly necessary.
\begin{comment}
- easy and cheap
- duplication of work
- re-exploring things
- nodes that were sent off to other PEs
- how to deal with global loop detection
- delete everything, too conservative, less performance
- do not delete, cut off things wrongly
- restarts deal with this increased false positive rate
\end{comment}

\paragraph{Communicate nothing}
Our third option in dealing with disappearing workers is to accept the loss of information and communicate nothing to the remaining PEs. This comes at the cost of loosing information while being easy to implement and allowing for immediate reassignment of PEs and avoiding any duplication of work. Additionally, we do not have to clear out the local fringe to avoid duplication, allowing for efficient re-assignment of PEs to their old job. \\
This leaves us with the problem of information loss. To deal with this, we can revisit the restart mechanism we introduced to deal with a similar problem in probabilistic loop detection. There we argued that correctly designed restarts would allow our planner to be complete even when using a loop detection mechanism suffering from false positives. For this we made no assumptions besides the false positive rater being less than 1. As Mallob guarantees that we will always have at least one PE, never loosing all information, we can simply model the loss of PEs and their local fringes as an extremely high false positive rate. From this it follows that restarts allow us to loose this local information while maintaining overall completeness.

\paragraph{Conclusion}
As we have seen, there are multiple approaches on how to handle a reduction in the number of available PEs while maintaining planner completeness. In our design, we decided to communicate the root node of our local search space to a random other PE, inserting it at the back end of the fringe. The other PE is chosen at random to avoid turning any single PE into a bottleneck. We choose this design, as it allows us to avoid the large overhead of communicating the whole fringe while not being overly reliant on restarts to achieve completeness. While restarts do offer us completeness from a theoretical perspective, times between restarts increase rapidly as run time increases. As such we are unwilling to loose large parts of our search space and instead allow for the risk of performing duplicate work. In addition to this, we keep the global loop detection information unchanged, hoping that even after a PE is no longer assigned to a job, other PEs will profit from the information it provides.
\begin{comment}
- keep the search space around
- restarts offer completeness from a theoretical perspective
- however, in practice it may take a long time
- we are not willing to loose potentially very large parts of our search space
\end{comment}
			
\subsubsection{Handling Lost Messages}
In the moldable version of CrowdHTN, we could made a fundamental assumption about all messages, namely that they were guaranteed to be delivered. In malleable CrowdHTN this is no longer possible. If PEs $p_1$, $p_2$ are both assigned to the same job, then $p_1$ may send a message to $p_2$ with $p_2$ being reassigned to a different job before the message arrives. Mallob deals with this by recognizing the message can no longer be handled and returning it to the sender. We go over the way we handle such return messages in section \ref{impl: malleable messages} our implementation chapter. \\
However, the changing assignments of PEs to jobs imply an additional problem. The return message may be lost as well, if the original sender gets assigned to a different job before the return message can be received. Mallob provides no further handling mechanism for this case. One way to solve this problem would be to extend Mallob to forward such a message to the job's root PE. In our design we instead chose to not handle this case for three reasons. \\
First, this case is highly contrived and we expect it to be of very little practical significance. It relies on a very specific sequence of events and could only present a problem if the message in question contained a work package. Second, by not handling this any further we simplify our design and implementation as we avoid special-casing the root PE. Third, as we choose to handle unassigned PEs by preserving the root of their local search space, information is preserved even if any of it's transitive children is lost in the moment. Due to this, the lost message does not represent a lost part of our search space. \\
We can further construct a case where a PE receives a work package and immediately passes it on due to a work request, subsequently loosing the search node. In this case we can still fall back to our restart mechanism to preserve completeness. In addition, we expect this to be extremely rare.
\begin{comment}
- this case is contrieved
- avoid turning the root into a special case
- information loss occurs specifically when a work package is lost
- we already communicate the root back
- this is no problem

- loosing information due to lost messages:
- Mallob has a mechanism to return messages
- however, messages can still be lost (return message and original sender dies in the meantime)
- we could change Mallob to send such messages to the root worker
- however, this would turn the root into a bottleneck
- again, we have mechanisms in place to deal with overall loss of information without loosing completeness
- at the same time, we need to adapt our handling of return messages to ensure all workers stay in a valid state and do not get stuck
- the worker may be replaced

- getting wrong information (worker dies, is replaced, gets message meant for old worker)

- we loose the ability to detect UNPLAN
\end{comment}
