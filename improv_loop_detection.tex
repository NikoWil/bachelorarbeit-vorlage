\subsection{Loop Detection}
\label{improv: loop detection}

\todo{Make sure to always talk about isomorphic task networks instead of equivalent? Quote \cite{holler2021loop}. Define isomorphism!}
\todo{quote about loop detection in graph search?}
\todo{quote about distributed loop detection in graph search?}
This section will discuss loop detection as it is used in (TO)HTN planning in general. It will start with an overview over loop detection in other HTN planners in section \ref{ld - history others}. This is followed by a discussion of the simplifying assumptions we can make for TOHTN planning in section \ref{ld - tohtn simplifications}.
- distributed loop detection (communication and merge operations become important!)
- perfect loop detection
- probabilistic loop detection (approximate membership query)

- \cite{magnaguagno2020hypertension} domains can have introduce infinite loops, without some kind of loop detection we loose completeness (and performance)


\subsubsection{Loop Detection in Other HTN Planners}
\label{ld - history others}
Loop detection in HTN planning is a recent phenomenon and was introduced in 2020 by the HyperTensioN planner (\cite{magnaguagno2020hypertension}) with the so-called 'Dejavu' technique. Dejavu works by extending the planning problem, introducing primitive tasks and predicates that track and identify when a particular recursive compound task is being decomposed. These new primitive tasks are invisible to the user. Information about recursive tasks is stored externally to the search as to not loose it during backtracking. Dejavu comes with performance advantages and protects against infinite loops. However, as Dejavu only concerns itself with information about the task network but ignores the world state it may have false positives. This was also noted by \cite{holler2021loop} and means that HyperTensioN is not complete. \cite{holler2021loop} further nodes that the loop detection is limited in that it only finds loops in a single search path but cannot detect if multiple paths lead to equivalent states. \\
In response to \cite{magnaguagno2020hypertension}, loop detection was introduced to the PANDA planner in \cite{holler2021loop}. Similar to HyperTensioN, PANDA keeps it's loop detection information separate in a list of visited states, $\mathcal{V}$. Search nodes, identified by a tuple $(s, tn)$ of world state $s$ and task network $tn$, are only added to the fringe if they are not contained in $\mathcal{V}$. To speed up comparisons, $\mathcal{V}$ is separated into buckets according to a hash of $s$. To then identify whether $tn \in \mathcal{V}[s]$ multiple algorithms are proposed. In the sub-case of TOHTN planning, both an exact comparison of the sequence of open tasks as well as an order-independent hash of the open tasks, called \textit{taskhash} are used. Similar to HyperTensioN, using a hash to identify equal task networks can lead to false positives and an incomplete planner. Both hashing-based and direct comparison as used in PANDA have a performance cost linear in the size of $tn$. The loop detection in PANDA improves upon the one in HyperTensioN insofar as it is able to detect loops where equivalent states where reached independently.

\begin{comment}
- loop detection in HTN planning is relatively new, being introduced by \cite{magnaguagno2020hypertension} and \cite{holler2021loop}
- multiple other planners do perform loop detection
- HyperTensioN \cite{magnaguagno2020hypertension}
	- transform the domain, adding primitive tasks that mark when a particular, recursive, task is being decomposed
	- use an external cache to store this information (else it would be lost in backtracking)
	- can also fall back on comparing the full search state (needed, if no parameters are present in the recursive task, to identify specific instances?)
	- limited, as it does not consider the world state, only the decompositions!, planner no longer complete \cite{holler2021loop}
- PandaGBFS \cite{holler2021loop}
	- search nodes are stored in a fringe
	- separate visited list $\mathcal{V}$
	- only add nodes to the fringe if they are not in $\mathcal{V}$
	- a node is identified by a pair $(s, tn)$, $s$ state, $tn$ task network (as in crowd!)
	- $\mathcal{V}$ is separated in buckets identified by states, access bucket based on 64 bit integer hashing (of $s$?)
	- problems with task network isomorphism, as Panda deals with HTN instead of only TOHTN, ordering relations matter!
	- simplifying isomorphism -> accept false positives (but not false negatives!), may cost completeness
	- exact isomorphism for TOHTN (TN -> sequence of tasks, simple)
	- hash (for TOHTN) that discards ordering information (nicer for HTN), only tracks multiplicity of all open tasks
	- hash can be computed in linear time (as no ordering)
	- hash is used both for making search more efficient (less task network comparisons!) and as an overapproximation
\end{comment}

\subsubsection{Assumptions in Loop Detection for CrowdHTN}
\label{ld - tohtn simplifications}
To design the loop detection in CrowdHTN, we have both simplifying and complicating assumptions that we will discuss here. \\
While both PANDA and HyperTensioN are HTN planners, CrowdHTN concerns itself only with TOHTN planning. As a result, the remaining task network can be represented as a sequence of open tasks with the ordering constraints implicit in how the sequence is stored.
\todo{refer to the implementation part where the open tasks are represented as a sequence} \\
Crowd identifies search states as a tuple of $(s, tn)$ of world state $s$ and task network $s$, similar to PANDA and uses hashing to efficiently identify duplicate search states. As tasks of equivalent task networks are always in the same order, we can incorporate that order into our hash of $tn$ to reduce the number of collisions compared to PANDA's \textit{taskhash}. This will increase performance where we fall back to comparisons in case of collisions and reduce our false-positive rate in case we use probabilistic loop detection. 
\todo{Refer to the implementation and how we reduce hashing to O(1) time} \\
Both PANDA and HyperTensioN are sequential planners whereas CrowdHTN is highly parallel. This adds an additional design constraint to our loop detection. If we want to efficiently share information about visited states using the full state information becomes infeasible as those full states would have to be communicated. If we perform loop detection only locally, we predict to suffer from decreased performance the higher the degree of parallelism. I.e., if two branches of our search tree contain the same search node, the chance that those two branches are encountered on different processors is higher the more processors we have.
\begin{comment}
	- keep the part where we store loop detection information externally
	- be sure we can always assume total order (and keep the ordering information to reduce false-positive rate!)
	- keep completeness of the planner
	- improve on the performance for hashing!
	- 
\end{comment}


\subsubsection{Perfect Loop Detection}
One simple way to perform loop detection which is similarly used in PANDA (\cite{holler2021loop}) is to use a hashset of visited states. The implementation in Crowd is slightly different from PANDA in that we use one combined hash for both world state and open tasks. Other than PANDA, CrowdHTN does incorporate the order of tasks into the hash, which should reduce collisions and makes the two levels of hashing less needed. \\
Using hashes combined with a full comparison provides us a perfect loop detection, i.e., neither false positives nor false negatives exist. This makes it a useful technique to benchmark other loop detection methods. However, both in the sequential and in the distributed case this technique suffers from performance problems. \\
In case of hash collisions, we have to fall back to a full comparison of world state $s$ and open tasks $tn$. While $s$ is bound in size by the total number of predicates, the size of $tn$ is effectively unbound \todo{quote about max. exponential depth!}. Additionally, we have to keep both $s$ and $tn$ around for all nodes ever encountered, increasing the memory footprint of our program.
In case of distributed loop detection, communicating full states would lead to a large communication overhead. Especially, we can expect each node to be larger than those sent as work packages, as those are optimized to have a small $tn$ which would not hold for nodes encountered in our loop detection.
\begin{comment}
	- Additionally, there is the full comparison for the world state
	- in practise this can be a problem, at the same time asymptotically it should not matter as it will be dwarfed by the 
	\todo{get some data on how many nodes share a world state on average as well as sizes of world states -> number of hash operations per world state!}
	- and the full comparison of open tasks
	- we cannot just free the open tasks and world state that are no longer needed! Both time and memory footprint are worse
	- worse memory and time complexity
	- however, it is a useful benchmark as to how many loops we *should* expect
\end{comment}

\subsubsection{Approximate Loop Detection}
\todo{Approximate Membership Queries}
\todo{Write about why a quotient filter is not the solution! (or at least why it was not chosen)}

\paragraph{Approximate membership queries}
- we drop the assumption of perfection
- allow either false positives or false negatives
- in our case false positives (i.e. only 'definitely not in filter' or 'probably in filter')
- the data structure we search is called approximate membership query
- bloom filters and quotient filters are two examples of this
- bloom filters use $k$ hash functions and set corresponding bits
- more compact representation of the whole thing
- checking for membership in O(1) instead of O(number of open tasks + world state size)

- expanding bloom filters allow us to mostly keep performance while allowing arbitrary size
- double size each time the filter is full

- detailed explanation of bloom filter

\paragraph{Completeness in the face of false positives}
- false positives mean we loose completeness
- i.e. if we perform progression search and $p = p_0, \ldots, p_n$ defines a path of search nodes from initial node to goal
- then for any $p_i$ in $p$, search nodes $p_0, \ldots, p_{i-1}$ may collectively set the $k$ hashes produced by $p_i$, filtering it out
- the chance of all paths to a goal node being filtered out as false positives only increases with each path we wrongly explore
- overall, we loose completeness

- one way to get completeness back: restarts while changing the seeds of the hash function
- assuming perfect hashing, for different seeds our hashes are completely uncorrelated
- as the number of restarts increases, the chance of encountering a false positive on the path start - goal every single time goes to zero
- only an infinite number of restarts as runtime goes to infinity does guarantee us this property
- we need arbitrarily long runs in between restarts, as plans may be arbitrarily long
	- to be precise, we could limit plan length by the combination of maximum depth before a plan must exist (if it does at all) and maximum expansion factor (i.e. max. number of children of any task)
	- simply allowing runs of any length simplifies the implementation, though
- we choose to check for a restart each second and, at second $t$, do it with probability $1 / t$
- for expected number of restarts we get $\sum_{t=1}^{\infty} \frac{1}{t}$
	- this is a geometric series which diverges
	- average time between restarts increases as time progresses
	- both properties are what we want!


\paragraph{Global Loop Detection}
- normal loop detection gets worse in distributed (TO)HTN planning
- each worker tracks it's own set of known nodes
- if worker $w_1$ encounters a node, workers $w_2, \ldots, w_n$ will not filter that node out until they've encountered it themselves at least once

- we try to find a way to perform global loop detection

- bloom filters have additional advantages:
	- smaller size makes them efficient to communicate
	- also, encoding and decoding for communication is trivial as the filter consists of a simple bit vector
	- bloom filters can be merged efficiently via bitwise operations
- bloom filters are shared between all workers by regularly performing a distributed reduction operation with a subsequent broadcast afterwards
- this takes logarithmic number of steps in the number of workers
- this gives us some global loop detection capabilities back
- we approximate the number of search nodes contained in the global filter as the sum of nodes in our communicated filters, this gives us a conservative upper bound which guarantees that our bounds for false positives are not violated

- we need to adjust our bloom filter behavior for this case
- locally we used expanding bloom filters, this is easy
- distributed, this is difficult
	- our workers may not be alive for equivalent time spans
	- as a result, the number of nodes in our shared global filter may vary between nodes
	- having a filter consist of differently sized sub-filters as in expanding bloom filters would complicate things here
	- workers with differently filled filters may disagree about the size filter into which to put a new search node
	- the implementation would get more complicated
	- also, we may loose our guarantees about false positives as our different workers will independently fill their filters and then combine them, producing a new filter that may contain more search nodes than permitted
	
	- so instead, when the global filter is full (aka false positive chance is too high), we force another restart
	- for this, we also double the size of our global filter, to limit the number of restarts that happen
	- and also allow arbitrarily long plans
- the root worker can control the restarts
- we know that the root is always alive for the whole duration of the job
- aka the root is guaranteed to partake in every single exchange of loop detection information
- aka the root's global filter is always at least as full as every other worker's filter
- so the root can do this independently, simplifying our implementation

- to not let the number of restarts take over, we also want to limit our global loop detection to important search nodes
- for now, we define important as encountering a search node which we've encountered before

\begin{comment}
Quotient filter:
- do we need the original elements to re-insert them?
- do we need to communicate whole hashes to combine filters? (worse communication size!)


Advantages:
- communication takes less effort (a lot lower size of data!)
- hashing in O(1)? Or at least in a lot easier
- we can pre-hash the open tasks (can be of exponential depth (requires a good argument, as the path down the task network could be of width 1?) \todo{do the maths}), thus turning hashing into O(1)
- in case of hash collisions we need to walk the whole sequence of open tasks
- in practice this is even worse, as our open tasks are saved in a tree-like manner to allow sharing of the tasks between search nodes
- this means we wildly jump through memory for hashing, adding another constant factor
- we could save on this constant factor by duplicating the open tasks for each node and saving them sequentially, but leading to a lot higher memory footprint (might be quadratic compared to what it is already (if any fixed fraction of nodes have at least 2 children, maybe? \todo{some math}))
- compromise between performance and false-positive rate (better for correctness than just comparing a single hash)
\end{comment}
