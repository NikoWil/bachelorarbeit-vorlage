\subsection{Loop Detection}
\label{improv: loop detection}
In recent years it has become clear that the recursive nature of (TO)HTN instances poses its own set of challenges to (TO)HTN planners. As a result, mechanisms to perform loop detection have become an active area of research with both HyperTensioN \cite{magnaguagno2020hypertension} and PANDA \cite{holler2021loop} exploring it.
In this section we will discuss loop detection specifically in the context of parallel and distributed (TO)HTN planning. We start out with a discussion of loop detection techniques in other planners such as PANDA and HyperTensioN in section \ref{ld - history others}. This is followed by a short overview of how CrowdHTN specifically differs and how this changes our base assumptions in section \ref{ld - tohtn simplifications}. Afterwards we first explore loop detection as it is already present in CrowdHTN in section \ref{ld - perfect loop detection}. We conclude by exploring how approximate-membership-query (AMQ) data structures can be used in (TO)HTN loop detection, how this affects completeness of the progression search algorithm and present our design for a distributed and global loop detection mechanism in section \ref{ld - approximate loop detection}.

\begin{comment}
- loop detection in other planners
- changes in CrowdHTN as a parallel planner
- loop detection based on hash sets
- what happens if we allow false positives
- implications on completeness (and how we deal with it)
- designing a distributed loop detection mechanism with information sharing

\todo{Make sure to always talk about isomorphic task networks instead of equivalent? Quote \cite{holler2021loop}. Define isomorphism!}
\todo{quote about loop detection in graph search?}
\todo{quote about distributed loop detection in graph search?}
This section will discuss loop detection as it is used in (TO)HTN planning in general. It will start with an overview over loop detection in other HTN planners in section \ref{ld - history others}. This is followed by a discussion of the simplifying assumptions we can make for TOHTN planning in section \ref{ld - tohtn simplifications}.
- distributed loop detection (communication and merge operations become important!)
- perfect loop detection
- probabilistic loop detection (approximate membership query)
\end{comment}

\subsubsection{Loop Detection in Other (TO)HTN Planners}
\label{ld - history others}
Loop detection in HTN planning is a recent phenomenon and was introduced in 2020 by the HyperTensioN planner (\cite{magnaguagno2020hypertension}) with the so-called 'Dejavu' technique. Dejavu works by extending the planning problem, introducing primitive tasks and predicates that track and identify when a particular recursive compound task is being decomposed. These new primitive tasks are invisible to the user. Information about recursive tasks is stored externally to the search as to not loose it during backtracking. Dejavu comes with performance advantages and protects against infinite loops. However, as Dejavu only concerns itself with information about the task network but ignores the world state it may have false positives. This was also noted by \cite{holler2021loop} and means that HyperTensioN is not complete. \cite{holler2021loop} further nodes that the loop detection is limited in that it only finds loops in a single search path but cannot detect if multiple paths lead to equivalent states. \\
In response to \cite{magnaguagno2020hypertension}, loop detection was introduced to the PANDA planner in \cite{holler2021loop}. Similar to HyperTensioN, PANDA keeps it's loop detection information separate in a list of visited states, $\mathcal{V}$. Search nodes, identified by a tuple $(s, tn)$ of world state $s$ and task network $tn$, are only added to the fringe if they are not contained in $\mathcal{V}$. To speed up comparisons, $\mathcal{V}$ is separated into buckets according to a hash of $s$. To then identify whether $tn \in \mathcal{V}[s]$ multiple algorithms are proposed. In the sub-case of TOHTN planning, both an exact comparison of the sequence of open tasks as well as an order-independent hash of the open tasks, called \textit{taskhash} are used. Similar to HyperTensioN, using a hash to identify equal task networks can lead to false positives and an incomplete planner. Both hashing-based and direct comparison as used in PANDA have a performance cost linear in the size of $tn$. The loop detection in PANDA improves upon the one in HyperTensioN insofar as it is able to detect loops where equivalent states where reached independently.
\begin{comment}
- loop detection in HTN planning is relatively new, being introduced by \cite{magnaguagno2020hypertension} and \cite{holler2021loop}
- multiple other planners do perform loop detection
- HyperTensioN \cite{magnaguagno2020hypertension}
	- transform the domain, adding primitive tasks that mark when a particular, recursive, task is being decomposed
	- use an external cache to store this information (else it would be lost in backtracking)
	- can also fall back on comparing the full search state (needed, if no parameters are present in the recursive task, to identify specific instances?)
	- limited, as it does not consider the world state, only the decompositions!, planner no longer complete \cite{holler2021loop}
- PandaGBFS \cite{holler2021loop}
	- search nodes are stored in a fringe
	- separate visited list $\mathcal{V}$
	- only add nodes to the fringe if they are not in $\mathcal{V}$
	- a node is identified by a pair $(s, tn)$, $s$ state, $tn$ task network (as in crowd!)
	- $\mathcal{V}$ is separated in buckets identified by states, access bucket based on 64 bit integer hashing (of $s$?)
	- problems with task network isomorphism, as Panda deals with HTN instead of only TOHTN, ordering relations matter!
	- simplifying isomorphism -> accept false positives (but not false negatives!), may cost completeness
	- exact isomorphism for TOHTN (TN -> sequence of tasks, simple)
	- hash (for TOHTN) that discards ordering information (nicer for HTN), only tracks multiplicity of all open tasks
	- hash can be computed in linear time (as no ordering)
	- hash is used both for making search more efficient (less task network comparisons!) and as an overapproximation
\end{comment}

\subsubsection{Assumptions in Loop Detection for CrowdHTN}
\label{ld - tohtn simplifications}
To design the loop detection in CrowdHTN, we have both simplifying and complicating assumptions that we will discuss here. \\
While both PANDA and HyperTensioN are HTN planners, CrowdHTN concerns itself only with TOHTN planning. As a result, the remaining task network can be represented as a sequence of open tasks with the ordering constraints implicit in how the sequence is stored.
\todo{refer to the implementation part where the open tasks are represented as a sequence} \\
Crowd identifies search states as a tuple of $(s, tn)$ of world state $s$ and task network $s$, similar to PANDA and uses hashing to efficiently identify duplicate search states. As tasks of equivalent task networks are always in the same order, we can incorporate that order into our hash of $tn$ to reduce the number of collisions compared to PANDA's \textit{taskhash}. This will increase performance where we fall back to comparisons in case of collisions and reduce our false-positive rate in case we use probabilistic loop detection. 
\todo{Refer to the implementation and how we reduce hashing to O(1) time} \\
Both PANDA and HyperTensioN are sequential planners whereas CrowdHTN is highly parallel. This adds an additional design constraint to our loop detection. If we want to efficiently share information about visited states using the full state information becomes infeasible as those full states would have to be communicated. If we perform loop detection only locally, we expect to suffer from decreased performance the higher the degree of parallelism. I.e., if two branches of our search tree contain the same search node, the probability that those two branches are encountered on different processors is higher the more processors we have.
\begin{comment}
	- keep the part where we store loop detection information externally
	- be sure we can always assume total order (and keep the ordering information to reduce false-positive rate!)
	- keep completeness of the planner
	- improve on the performance for hashing!
\end{comment}

\subsubsection{Perfect Loop Detection}
\label{ld - perfect loop detection}
One simple way to perform loop detection which is similarly used in PANDA (\cite{holler2021loop}) is to use a hashset of visited states. The implementation in Crowd is slightly different from PANDA in that we use one combined hash for both world state and open tasks. Other than PANDA, CrowdHTN does incorporate the order of tasks into the hash, which should reduce collisions and makes the two levels of hashing less needed. \\
Using hashes combined with a full comparison provides us a perfect loop detection, i.e., neither false positives nor false negatives exist. This makes it a useful technique to benchmark other loop detection methods. However, both in the sequential and in the distributed case this technique suffers from performance problems. \\
In case of hash collisions, we have to fall back to a full comparison of world state $s$ and open tasks $tn$. While $s$ is bound in size by the total number of predicates, the size of $tn$ is effectively unbound \todo{quote about max. exponential depth!}. Additionally, we have to keep both $s$ and $tn$ around for all nodes ever encountered, increasing the memory footprint of our program.
\begin{comment}
	- Additionally, there is the full comparison for the world state
	- in practise this can be a problem, at the same time asymptotically it should not matter as it will be dwarfed by the 
	\todo{get some data on how many nodes share a world state on average as well as sizes of world states -> number of hash operations per world state!}
	- and the full comparison of open tasks
	- we cannot just free the open tasks and world state that are no longer needed! Both time and memory footprint are worse
	- worse memory and time complexity
	- however, it is a useful benchmark as to how many loops we *should* expect
\end{comment}

\subsubsection{TODO: Approximate Loop Detection}
\label{ld - approximate loop detection}
- so far: loop detection is perfect
- what if we drop this assumption of no false positives?
- short overview over AMQ data structures, focus on bloom filters
- explore the implications on completeness
- how does this help us with distributed loop detection?
\todo{Approximate Membership Queries}
\todo{Write about why a quotient filter is not the solution! (or at least why it was not chosen)}

\paragraph{TODO: Approximate membership queries}
\cite{bender2011don}
- approximate-membership-query (AMQ) data structures
- introduce quotient filter

\cite{fan2014cuckoo}
- introduce cuckoo filters

\cite{bloom1970space}
- introduces bloom filters as a concept
- optimized for easy rejection
- admits some false positives
- bloom filter is an array of $m$ bits, initially all set to 0
- use $k$ hash functions
- inserting an element: compute $k$ hashes which are in $0, \ldots, m - 1$ and set the corresponding bits to 1
- testing for element: check all corresponding bits
- assuming hashing takes time $\mathcal{O}(t)$, this gives us time $\mathcal{O}(k \cdot t)$ for insertion and membership query

- bloom filters do not support deletions as we cannot simply set a bit to zero (more than one element may have set it to 1)
\cite{fan2000summary}
- instead of a single bit use a counter, can be decreased in all $k$ locations for deletion

\cite{broder2004network}
- false positive rate is $(1 - e^{-\frac{kn}{m}})^k$

\cite{xie2007scalable}
- bloom filters assume a static set
- technically we do have a static set, however it is prohibitively large (and we will never fully explore it), as a result we treat it as dynamic
- defines scalable bloom filters: add a new filter of twice the size when full
- after adding a linear number of filters, we can store an exponential number of elements while keeping our error guarantees
- insertion: check whether biggest filter is full, if yes, create new else insert into biggest
- membership check: look up in all sub-filters

\paragraph{Completeness in the face of false positives}
While using approximate membership queries as described in the previous paragraph gives us a number of advantages, it also introduces a new set of challenges. Among these is the fact that false positives become a possibility. As a result, we can expect to wrongfully cut off parts of our search space. More specifically, if we perform progression search using bloom filters and $n = n_1, \ldots, n_l$ is a path of length $l$ from our initial search node to a goal search node, then for any $n_i$ in $n$, the search nodes $n_1, \ldots, n_{i-1}$ may collectively set the $k$ hashes associated with $n_i$, filtering it out. This may end up turning a (TO)HTN problem unsolvable for us even though a plan exists. As a result, our planner, if otherwise unchanged, will no longer be complete. \\
We will now take a closer look at the probabilities involved. Assuming we use an expanding bloom filter with maximum false positive rate $p < 1$ and a hash function which maps search nodes to uniformly independent values. Let us further assume we perform progression search on a solvable (TO)HTN problem where the shortest path from start to goal is of length $l$. Then $(1 - p)^l < 1$ is an upper bound on the probability that we are unable to solve the problem even though a solution exists. \\
From this we see that $\lim_{l \rightarrow \infty} (1 - p)^l \rightarrow 0$. That is, if keep re-running our search with new, independent hash functions we regain completeness. It is critical to use independent hash functions between runs to ensure that false positives in different runs are not correlated with each other. In the next step we need to determine when ot re-run our planner. There are three main constraints.
\begin{enumerate}
	\item As the (TO)HTN instance may be recursive, the search space may be infinite
	\item The number of restarts needs to be unbounded as run time goes to infinity
	\item As a plan may be arbitrarily long, the number of runs with run time at least $t$ needs to be infinite
\end{enumerate}
Constraint one implies that we cannot simply wait until we explore the whole search space before restarting. Instead we will base our restarts on total run time so far. To fulfill constraints two and three, we perform a check every second where after $t$ seconds we perform a restart with probability $\frac{1}{t}$. \\
For the total number of restarts we end up with $\sum_{t = 1}^\infty \frac{1}{t}$. This is the harmonic series and diverges, giving us the required unbounded number of restarts. As $t$ grows, the probability of restarts decreases, allowing for increasingly long runs, fulfilling the third constraint. \\
This mechanism allows us to restore completeness to our planner while utilizing approximate loop detection. Restarts may prove to have additional benefits to planner performance, as DFS-based planners tend to be hit-or-miss where restarts increase the number of opportunities for a hit. We do note that approximate loop detection does come at the cost of no longer being able to detect UNPLAN, as we can never guarantee that we explored the full search space. In practice, we do not expect this to matter as the search space of (TO)HTN problems tends to be too big to feasibly fully explore.
\begin{comment}
- false positives mean we loose completeness
- i.e. if we perform progression search and $p = p_0, \ldots, p_n$ defines a path of search nodes from initial node to goal
- then for any $p_i$ in $p$, search nodes $p_0, \ldots, p_{i-1}$ may collectively set the $k$ hashes produced by $p_i$, filtering it out
- the chance of all paths to a goal node being filtered out as false positives only increases with each path we wrongly explore
- overall, we loose completeness

- one way to get completeness back: restarts while changing the seeds of the hash function
- assuming perfect hashing, for different seeds our hashes are completely uncorrelated
- as the number of restarts increases, the chance of encountering a false positive on the path start - goal every single time goes to zero
- only an infinite number of restarts as runtime goes to infinity does guarantee us this property
- we need arbitrarily long runs in between restarts, as plans may be arbitrarily long
- to be precise, we could limit plan length by the combination of maximum depth before a plan must exist (if it does at all) and maximum expansion factor (i.e. max. number of children of any task)
- simply allowing runs of any length simplifies the implementation, though
- we choose to check for a restart each second and, at second $t$, do it with probability $1 / t$
- for expected number of restarts we get $\sum_{t=1}^{\infty} \frac{1}{t}$
- this is a geometric series which diverges
- average time between restarts increases as time progresses
- both properties are what we want!
\end{comment}

\paragraph{Global Loop Detection}
In the previous section \ref{ld - tohtn simplifications} we already mentioned that loop detection in distributed (TO)HTN planning comes with unique problems. Specifically, current loop detection techniques do not include ways to efficiently share the loop detection information between PEs. As a result, a search node is only fully filtered out once each worker has encountered it at least once. This is less of a problem for recursive tasks which are quickly re-encountered by our search and subsequently filtered out. It does however loose us the ability to discover when different paths lead to equivalent nodes in which case we want to avoid duplicate work. To address this issue, we will start with a short discussion on how previous loop detection mechanisms are hard to adapt for the distributed case and then show how we implement distributed loop detection on the basis of bloom filters. \\
As mentioned in section \ref{ld - perfect loop detection} on current loop detection mechanisms, they do suffer from a high memory footprint as we keep whole search nodes around. This problem extends to the distributed case, as we would now have to communicate whole search nodes leading to a large message overhead. Even if we assume the communication overhead to be low enough, we run into additional problems. Inserting $n$ elements into a hashset takes $\mathcal{O}(n)$ time. The higher the number of PEs, the more time would be spent on inserting search nodes received from other PEs which would either block us from performing search for large amounts of time or introduce synchronization problems. As a result, we have decided that it is infeasible to extend this loop detection mechanism to the distributed case. \\
\todo{we also implemented mechanisms to save on memory by sharing data between nodes. Additional encoding and decoding overhead!}
\begin{comment}
- insertion of $n$ search nodes is expensive
- encoding and decoding becomes more difficult, especially


- previous section \ref{ld - tohtn simplifications} mentions that distributed planning makes loop detection less efficient
- recursion is still fine
- different paths leading to equivalent nodes are no longer detected

- normal loop detection gets worse in distributed (TO)HTN planning
- each worker tracks it's own set of known nodes
- if worker $w_1$ encounters a node, workers $w_2, \ldots, w_n$ will not filter that node out until they've encountered it themselves at least once

- we try to find a way to perform global loop detection
\end{comment}
Compared to hash sets, loop detection based on bloom filters offers a number of advantages for distributed loop detection. First, bloom filters offer a more compact representation of the already encountered nodes which leads to a lower communication overhead. Second, as the filter itself is stored as a simple bit vector, we have negligible overhead regarding encoding and decoding for communication. Thirdly, the merging operation of two bloom filters is very efficient. To merge two bloom filters, we perform a simple bitwise or operation of the bit vectors. In a combined filter, we get a conservative upper bound for the total number of contained elements by adding the number of elements of both filters. This guarantees that our maximum rate of false positives is not exceeded. \\
\begin{comment}
- bloom filters have additional advantages:
- smaller size makes them efficient to communicate
- also, encoding and decoding for communication is trivial as the filter consists of a simple bit vector
- bloom filters can be merged efficiently via bitwise operations
- bloom filters are shared between all workers by regularly performing a distributed reduction operation with a subsequent broadcast afterwards
- this takes logarithmic number of steps in the number of workers
- this gives us some global loop detection capabilities back
- we approximate the number of search nodes contained in the global filter as the sum of nodes in our communicated filters, this gives us a conservative upper bound which guarantees that our bounds for false positives are not violated
\end{comment}
To integrate bloom filters into our distributed loop detection, we also need to address the question of what to do in case the global filter gets filled up, i.e. it's false positive rate reaches our set limit. For the local case, we introduced expanding bloom filters.
This is a problem as we now communicate whole sets of search nodes whereas locally we introduce new search nodes into the filter one by one, increasing the size at exactly the right moment. As a result, we face the choice of either throwing away some information or loosing our guarantees regarding false positives. Similarly, we face the problem where different PEs may disagree about the current maximum size of the expanding bloom filter, putting more information in a smaller filter that will be thrown away by other PEs. \\
To deal with this problem, we induce a restart in our search upon filling our global filter. As the restart mechanism is already present due to the need to deal with false positives this is an easy adaption. To limit the number of needed restarts and once again allow arbitrarily long runs, we double the size of our global filter with each restart. This limits the number of restarts needed and increases the time between subsequent loop detection induced restarts, once again allowing for arbitrarily long runs to preserve completeness of our planner. Additionally, we limit the amount of information present in our filter to further reduce the number of restarts we need. We achieve this by only putting 'important' search nodes into our filter. Our heuristic to determine search node importance is to put a node into our global filter if it is present in our local filter and encountered again. Other heuristics are possible but beyond the scope of this thesis.
\begin{comment}
- we need to adjust our bloom filter behavior for this case
- locally we used expanding bloom filters, this is easy
- distributed, this is difficult
- our workers may not be alive for equivalent time spans
- as a result, the number of nodes in our shared global filter may vary between nodes
- having a filter consist of differently sized sub-filters as in expanding bloom filters would complicate things here
- workers with differently filled filters may disagree about the size filter into which to put a new search node
- the implementation would get more complicated
- also, we may loose our guarantees about false positives as our different workers will independently fill their filters and then combine them, producing a new filter that may contain more search nodes than permitted

- so instead, when the global filter is full (aka false positive chance is too high), we force another restart
- for this, we also double the size of our global filter, to limit the number of restarts that happen
- and also allow arbitrarily long plans
- the root worker can control the restarts
- we know that the root is always alive for the whole duration of the job
- aka the root is guaranteed to partake in every single exchange of loop detection information
- aka the root's global filter is always at least as full as every other worker's filter
- so the root can do this independently, simplifying our implementation

- to not let the number of restarts take over, we also want to limit our global loop detection to important search nodes
- for now, we define important as encountering a search node which we've encountered before
\end{comment}

\begin{comment}
Quotient filter:
- do we need the original elements to re-insert them?
- do we need to communicate whole hashes to combine filters? (worse communication size!)


Advantages:
- communication takes less effort (a lot lower size of data!)
- hashing in O(1)? Or at least in a lot easier
- we can pre-hash the open tasks (can be of exponential depth (requires a good argument, as the path down the task network could be of width 1?) \todo{do the maths}), thus turning hashing into O(1)
- in case of hash collisions we need to walk the whole sequence of open tasks
- in practice this is even worse, as our open tasks are saved in a tree-like manner to allow sharing of the tasks between search nodes
- this means we wildly jump through memory for hashing, adding another constant factor
- we could save on this constant factor by duplicating the open tasks for each node and saving them sequentially, but leading to a lot higher memory footprint (might be quadratic compared to what it is already (if any fixed fraction of nodes have at least 2 children, maybe? \todo{some math}))
- compromise between performance and false-positive rate (better for correctness than just comparing a single hash)
\end{comment}
